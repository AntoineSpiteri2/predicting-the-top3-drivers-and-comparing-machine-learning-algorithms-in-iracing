# predicting-the-top3-drivers-and-comparing-machine-learning-algorithms-in-iracing
The project is a bachelor’s‐level dissertation that investigates whether machine learning models can reliably predict the top three finishers (“podium positions”) in live iRacing competitions by combining both historical driver statistics and real-time telemetry. At a high level, the work proceeds in three main stages:  Data Collection & Integration  Historical Driver Data: Using the iRacing REST API, long‐term performance metrics (e.g., season points, average finishing positions, consistency measures) are fetched for every driver in each targeted race.  Real-Time Telemetry: During live races, high-frequency telemetry streams (vehicle speed, gap to leader, lap times, incident counts, etc.) are ingested from the iRacing SDK.  These two data sources are joined in a single hybrid pipeline so that, at any given moment in a race, the model sees both a driver’s past performance “baseline” and their most recent live signals. Evaluating_Machine_Lear…  Feature Engineering & Dataset Construction  Decayed & Bucketed Telemetry: Instead of feeding raw lap-by-lap numbers directly into the models, live telemetry features are transformed into “decay‐weighted buckets” (e.g., events occurring in the last 0–25%, 25–50%, 50–75%, and 75–100% of a driver’s current stint), ensuring that more recent information carries higher weight. For instance, “number of incidents in the last five laps” or “average gap to leader over the most recent bucket” become standardized input columns.  Custom Target Metric (“RankscoreAdjusted”): Rather than just predicting literal finishing position, the authors devise an adjusted ranking score that accounts for time/distance gaps and race context, making the learning problem more robust to “mid-lap” snapshots.  Combining Static & Dynamic Features: Ultimately, each training example (i.e., one snapshot of a driver mid-race) includes:  A vector of historical attributes (career win rate, previous season ranking, track familiarity, etc.).  A set of engineered live telemetry features (decayed bucket metrics, lap recency pulses, pit-stop offsets, incident counts, etc.).  Model Training, Evaluation & Deployment  Three different machine learning architectures are compared:  XGBoost (gradient-boosted decision trees).  Random Forest (bagged decision trees).  RankNet (a pairwise neural ranking model that directly optimizes ordinal relationships).  Models are trained on a flat-file dataset of over 800 race snapshots, each labeled with the “ground truth” podium order at the moment of capture. Performance is assessed on 18 held-out races using metrics such as top-3 accuracy (i.e., whether the model correctly identifies all three podium drivers), Spearman rank correlation (how well the predicted driver ordering matches the true ordering), and additional error‐based measures.  Key Findings:  RankNet attains the highest overall generalization, achieving roughly 97% accuracy in predicting the actual top three and exhibiting minimal overfitting between training and test sets.  XGBoost occasionally achieves perfect podium accuracy on certain snapshots but shows higher variance (i.e., sometimes overfits to structured patterns in the training data).  Random Forest is fast and relatively stable for point‐estimate tasks (e.g., exact finishing position), but its rank‐order consistency under real-time constraints is weaker compared to the other two.  Live Prediction Loop: An asynchronous inference pipeline is implemented so that, every 3 seconds during a live race, the chosen model (RankNet in production) can recompute and broadcast updated podium predictions. This proves fast enough for potential integration into live broadcasts or driver-assist dashboards.  Interpretability & Practical Implications  SHAP Analysis is used to identify which features most influence the model’s predictions. Variables like “normalized gap to leader,” “driver consistency score,” and “grid position” emerge as top predictors of eventual podium finishes.  The project highlights how combining historical performance data with real-time signals produces significantly more reliable mid-race forecasts than using either data source alone.  Potential Applications include:  A “data-driven coaching” tool that signals warning flags when lesser-known drivers suddenly become podium threats.  An esports broadcast overlay that continuously shows likely podium contenders for enhanced viewer engagement.  Strategic simulators or “virtual pit-wall” dashboards that teams can use to adjust their tactics mid-race (e.g., choosing whether to pit under caution based on opponent predictions).  Limitations & Future Work  The dataset covers a limited variety of weather conditions (e.g., mostly dry, no rain‐affected races), and some telemetry channels (like brake temperatures or tyre wear) were unavailable—potentially constraining model robustness under more diverse circumstances.  Future extensions could incorporate advanced sensor feeds (if iRacing ever exposes engine/tyre-wear data), widen the variety of tracks and ambient conditions, and experiment with alternative ranking architectures (e.g., Graph Neural Networks to model inter-driver interactions).  In summary, the project demonstrates that a carefully engineered, hybrid data pipeline—melding historic driver statistics with decayed, time-bucketed live telemetry—combined with a rank-based neural model (RankNet) can very accurately forecast the top three finishers in real time. This work lays a foundation for both academic research in esports analytics and practical tools for teams, commentators, and fans.
